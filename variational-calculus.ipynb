{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5fda7414",
   "metadata": {},
   "source": [
    "# Optimization and Calculus of Variations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f04701",
   "metadata": {},
   "source": [
    "## Optimization Primer\n",
    "\n",
    "Optimization is the science of finding the best way to do something. Unsurprisingly, it is prevalent in many fields of engineering, science, and finance. We want to find ways to maximize profits, minimize failures, et cetera.\n",
    "The entire field of Operations Reasearch is dedicated to solving optimization problems.\n",
    "\n",
    "In it's simplest form, an optimization problem is given as a function, and we seek the point at which this function becomes a minimum or maximum.\n",
    "\n",
    "**Example**: I hold two risky securities whose risky returns have variance $\\sigma_1$ and $\\sigma_2$. The securities are correlated with covariance $c$. What proportion of each security should I hold in order to minimize the variance?\n",
    "\n",
    "**Model**: Let $w$ be the fraction that I invest in security 1. Then $(1-w)$ is the fraction of cash invested in the second security. The total variance of this portfolio is $w^2\\sigma_1^2 + (1-w)^2\\sigma_2^2+2w(1-w)c$.\n",
    "To solve the problem, I must find $w$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ff3128",
   "metadata": {},
   "source": [
    "Let's define what we mean by optimility.\n",
    "\n",
    "### Defintion: Minimum\n",
    "Let $S$ be a set and $f: S \\rightarrow \\mathbb{R}$ a function from this set to the real numbers. $f$ has a **local minimum** at $x_0 \\in V$ if there exists a neighborhood of $U$ of $x_0$ such that $f(x) \\geq f(x_0) \\forall x \\in U$. \n",
    "$x_0$ is a **gobal minimum** if $f(x) \\geq f(x_0) \\forall x \\in S$.\n",
    "\n",
    "The definition for maximum is similar with the inequalities reversed. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f7b124",
   "metadata": {},
   "source": [
    "We know from calculus, that for a real valued function $f: \\mathbb{R} \\rightarrow \\mathbb{R}$, the derivative of the function is 0 at a local minimum or maximum, as well as on saddle points.\n",
    "\n",
    "If a local minimum occurs at $x_0$, then $f'(x_0) = \\dot{f}(x_0) ≡ \\frac{df}{dx}(x_0) = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8cb5c2",
   "metadata": {},
   "source": [
    "<img src='variations/Tangent_function_animation.gif'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467ad0c0",
   "metadata": {},
   "source": [
    "This property suggests a simple algorithm for finding the minimum: Find all points where the derivative of the function is 0. If there are multiple, compute the value of the fucntion at each one, and choose the minimum.\n",
    "\n",
    "Using this algorithm, we can now solve the variance minimization problem:\n",
    "\n",
    "$$\n",
    "f(w) = w^2\\sigma_1^2 + (1-w)^2\\sigma_2^2+2w(1-w)c\n",
    "$$\n",
    "\n",
    "Taking the derivative and doing a bit of algebra, we obtain:\n",
    "\n",
    "$$\n",
    "w_{min} = \\frac{\\sigma_2^2 - c}{\\sigma_1^2+\\sigma_2^2-2c}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4049097",
   "metadata": {},
   "source": [
    "This important theorem has a generalization in many dimensions for functions $f:  \\mathbb{R^n} \\rightarrow \\mathbb{R}$\n",
    "\n",
    "> If $f$ has a local minimum at $x_0$, then $\\nabla f(x_0) = 0$. \n",
    "\n",
    "The converse is not always true but if $f$ has a second derivative,\n",
    "a stronger condition exists to guarantee a minimum.\n",
    "\n",
    "> $f$ has a local minimum at $x_0$ if and only if, $\\nabla f(x_0) = 0$ and $\\nabla^2 f(x_0) \\geq 0$. \n",
    "\n",
    "Even though the algorithm for finding minima by computing the points of zero gradiend only guarantees a local minimum, it works so well that is has widespread applicatins. \n",
    "A large class of Machine Learning problems are formulated as multi-variate optimization problems that are solved by computing the value of zero gradient. When it comes to Deep Learning networks, this optimization involves space of tens of thousands, even millions, of dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e19cb6",
   "metadata": {},
   "source": [
    "### Example: Linear Regression\n",
    "\n",
    "Given a collection of data points $(x_i, y_i)$, fit a linear function $f(x) = a + bx$ in order to minimize the least square error\n",
    "\n",
    "$$\n",
    "E := \\frac{1}{N}\\sum\\limits_{i=1}^N (y_i - f(x_i))^2\n",
    "$$\n",
    "\n",
    "#### Solution\n",
    "We want to minimize $\\sum\\limits_{i=1}^N (y_i - a x_i + b))^2$, \n",
    "so we take derivatives with respect to $a$ and $b$ and set to 0.\n",
    "\n",
    "**TODO** Complete\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08fcae0f",
   "metadata": {},
   "source": [
    "We have a powerful tool for calculating optimality over a large number of dimensions. Yet, there are still problems that cannot be solved with this framework as developed so far.\n",
    "\n",
    "### Example: Surface of revolution\n",
    "Consider the following problem: I draw the graph of a function $f(x): \\mathbb{R} \\rightarrow \\mathbb{R}$ between two points $x_1$ and $x_2$. Then I revolve the graph around the x-axis to create a surface.\n",
    "The area of the surface thus described has area \n",
    "$$\n",
    "A = 2\\pi\\int\\limits_{x_1}^{x_2} x\\sqrt{1+f'(x)}dx\n",
    "$$\n",
    "\n",
    "We are interested in finding the function that results in the surface of revolution of minimal area."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89c8d94",
   "metadata": {},
   "source": [
    "## TODO: Illustration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baeb8fb9",
   "metadata": {},
   "source": [
    "<img src='variations/revolution.png'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be11f02",
   "metadata": {},
   "source": [
    "## The Derivative\n",
    "In the above example, we are not simply looking for a point in a high-dimensional space, but for an entire function defined on the interval $x_1 - x_2$. The are A is a function of functions. These are usually called **functionals**. \n",
    "\n",
    "In order to adopt the above optimization framework to work for functional, we will need to make some generalizations.\n",
    "The first thing to do is to look closely at the definition of the derivative and see how it can be extened.\n",
    "\n",
    "In a calculus class the derivative at point $x$ is usually defined as\n",
    "\n",
    "$$\n",
    "\\frac{df}{dx} := \\lim_{h \\rightarrow 0}\\frac{f(x+h)-f(x)}{h} \n",
    "$$\n",
    "\n",
    "Even in this simple one dimensional definition one has to be careful, because we may arrive at different result if we approach 0 from the left (h negative) or from the right (h positive).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86961725",
   "metadata": {},
   "source": [
    "<img src='variations/Absolute_value.png'  width='300'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9f5a8c",
   "metadata": {},
   "source": [
    "In more dimensions, the situation becomes even trickier. Each partial derivative may exist at a given point, but the function may still fail to be differentiable at that point. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb18986",
   "metadata": {},
   "source": [
    " **TODO**: Plot example of f with partial x and y but not differentiable along x = y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7064824",
   "metadata": {},
   "source": [
    "How can we tell if a function is trully differentiable without calculating derivatives in every direction? In advanced calculus, we find this definition.\n",
    "\n",
    "**Definition**  A function $f: \\mathbb{R}^n → \\mathbb{R}^m$ is *differentiable* at $x∈ \\mathbb{R}^n$ if there exists a linear transformation $D_x: \\mathbb{R}^n → \\mathbb{R}^m$ such that\n",
    "\n",
    "$$\n",
    "\\lim_{h\\rightarrow 0}\\frac{||f(x+h)-f(x)-D_x(h)||}{||h||} = 0\n",
    "$$\n",
    "\n",
    "So the derivative is not defined as a number or a vector but as a linear transformation. The form that the derivative takes in finite dimensional spaces is the Jacobian matrix.\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "  \\frac{\\partial f_1}{\\partial x_1} & \n",
    "    \\frac{\\partial f_1}{\\partial x_2} & \n",
    "    \\frac{\\partial f_1}{\\partial x_3} \\\\[1ex] % <-- 1ex more space between rows of matrix\n",
    "  \\frac{\\partial f_2}{\\partial x_1} & \n",
    "    \\frac{\\partial f_2}{\\partial x_2} & \n",
    "    \\frac{\\partial f_2}{\\partial x_3} \n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49eaaf66",
   "metadata": {},
   "source": [
    "TODO: Approximate the value of a function:\n",
    "\n",
    "$$\n",
    "f(x+a) = f(x)+\\nabla f(x)a+O(a^2)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c067c216",
   "metadata": {},
   "source": [
    "## Beyond finite dimensions\n",
    "\n",
    "Mathematician like to generalize everything and they have done the same with the derivative. Having defined derivatives in finite dimensional spaces, can we also define it in infinite dimensional?\n",
    "\n",
    "We first need some idea of what \"infinite dimensional\" means? To do this we need to use one of the most useful mathematical abstractions: The Vector space. \n",
    "\n",
    "Informally, a **Vector Space** is a set of objects, called **vectors** that can be added together to give other vectors. They can also be multiplied by real numbers to give other vectors.\n",
    "\n",
    "The intuition is our own space of 3 dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4017b180",
   "metadata": {},
   "source": [
    "<img src='variations/vector_2d_add.png' width='400'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5bb8ccc",
   "metadata": {},
   "source": [
    "but it works well for all sorts of other objects. For example, I can add real-valued functions together and get other real-valued functions. Also, if I multiply a real-valued function with a number, I get another real-valued function. So, real-valued functions are also vectors.\n",
    "\n",
    "Spaces of functions are pretty huge - much bigger than any finite dimension space of vectors. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111d5ba7",
   "metadata": {},
   "source": [
    "Our familiar vectors from$ R^n$ have concept of magnitude and distance. The magnitude of a vector $v$ is given by its norm $||v||$, whereas the distance between two vectors is given by the norm of their difference $||v-w||$. \n",
    "\n",
    "More general spaces, such as the space of all continuous real-valued functions, do not necessarily have natural concepts of magnitude or distane. \n",
    "\n",
    "However, in some cases, it is stil possible to define the concept of a norm in more general vector spaces. \n",
    "To make things conrete, let's define what we mean by \"norm\":\n",
    "\n",
    "### Definition: Norm\n",
    "A **norm** on a vector space V is a function $V->R$, such that:\n",
    "\n",
    "- ||x|| ≥ 0 \n",
    "- $||ax|| = |a|||x||$ (scales for real numbers a)\n",
    "- $||x + y|| ≤  ||x|| + ||y||$ (triangle inequality)\n",
    "- $||x|| = 0$   if and only if $x = 0$\n",
    "\n",
    "Example: L1 space\n",
    "\n",
    "$$\n",
    "||f|| = \\sqrt{\\int |f(x)|dx}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48009371",
   "metadata": {},
   "source": [
    "### The French Derivatives\n",
    "\n",
    "Having generalized the concepts of vector space and norm, we can now take the definition of the total derivative from multivariate calculus and adapt it to arbitrary normed vector spaces V and W.\n",
    "\n",
    "#### Definition: Frechet Derivative\n",
    "A function $f: V → W$ is *differentiable* at $x∈ V$ if there exists a linear transformation $D_x: V → W$ such that\n",
    "\n",
    "$$\n",
    "\\lim_{h\\rightarrow 0}\\frac{||f(x+h)-f(x)-D_x(h)||_W}{||h||_V} = 0\n",
    "$$\n",
    "\n",
    "In practive, the Frechet derivative is hard to compute\n",
    "\n",
    "#### Definition: Gateaux Derivative\n",
    "**TODO**\n",
    "\n",
    "$$\n",
    "\\lim_{e → 0} \\frac{f(x+eh)-f(x)}{e}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0c339d",
   "metadata": {},
   "source": [
    "## Calculus of Variations\n",
    "\n",
    "The *Calculus of Variations* is a special case of functional equations where the target vector space is just $\\mathbb{R}$. Luckily, it turns out that the conditions for optimality still hold, with some care, in this case:\n",
    "\n",
    "> If $x_0$ is a local minimum of $f:V → \\mathbb{R}$, then the Frechet derivative is 0 at $x_0$.\n",
    "\n",
    "As we mentioned above, it is sufficient to calculate the Gateaux derivative and make sure it is indepdent of the choice of \"direction\" h.\n",
    "\n",
    "### TODO: Example\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b0fb2c",
   "metadata": {},
   "source": [
    "### The Euler Lagrange Equations\n",
    "\n",
    "$$\n",
    "I(x) = \\int\\limits_{t_1}^{t_2} L(x, \\dot{x})dt\n",
    "$$\n",
    "\n",
    "Problem: Find x that minimizes I.\n",
    "\n",
    "This is an optimization problem in an infinite dimensional space. \n",
    "It turns out that the situation is simular to the finite dimensional cases. \n",
    "We look for places where the derivative of I is equal to 0.\n",
    "\n",
    "As we mentioned, the Frechet derivative is hard to compute by the definition. We compute the Gateaux derivative and if independent of the direction, it is the derivative.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90eadd8f",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{I(x+eh) - I(x)}{e} =\\\\\n",
    "\\int dt \\left(L(x+eh, \\dot{x}+e\\dot{h})-L(x,\\dot{x}) + O(e^2)\\right) = \\\\\n",
    "\\int dt e \\left(\\frac{\\partial L}{∂ x}h + \\frac{\\partial L}{∂ \\dot{x}}\\dot{h} \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f193f33b",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{d}{dt}\\left(\\frac{\\partial L}{∂ \\dot{x}}h\\right) = \n",
    "h\\frac{d}{dt}\\left(\\frac{\\partial L}{∂ \\dot{x}}\\right) + \\dot{h}\\frac{\\partial L}{∂ \\dot{x}} \\implies\\\\\n",
    "\\dot{h}\\frac{\\partial L}{∂ \\dot{x}} = \n",
    "\\frac{d}{dt}\\left(\\frac{\\partial L}{∂ \\dot{x}}h\\right)\n",
    "- h\\frac{d}{dt}\\left(\\frac{\\partial L}{∂ \\dot{x}}\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca46182b",
   "metadata": {},
   "source": [
    "Now we note that \n",
    "\n",
    "$$\n",
    "\\int\\limits_{t_1}^{t_2} dt \\frac{d}{dt}\\left(\\frac{\\partial L}{∂ \\dot{x}}h\\right)\n",
    "= \\frac{\\partial L}{∂ \\dot{x}}h(t_2) - \\frac{\\partial L}{∂ \\dot{x}}h(t_1)\n",
    "$$\n",
    "\n",
    "But $h(t_1) = h(t_2) = 0$ (why?) so the integral vanishes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823a97d2",
   "metadata": {},
   "source": [
    "We are left with\n",
    "\n",
    "$$\n",
    "\\frac{I(x+eh) - I(x)}{e} = ∫ dt h \\left[\n",
    "\\frac{\\partial L}{∂ x}\n",
    "- \\frac{d}{dt}\\left(\\frac{\\partial L}{∂ \\dot{x}}\\right) \n",
    "\\right]\n",
    "= 0\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ea6285",
   "metadata": {},
   "source": [
    "Since $h$ is arbitrary, we must have \n",
    "$$\n",
    "\\frac{\\partial L}{∂ x}\n",
    "- \\frac{d}{dt}\\left(\\frac{\\partial L}{∂ \\dot{x}}\\right)  = 0\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77de5950",
   "metadata": {},
   "source": [
    "### Example Surface of Revolution of minimal area\n",
    "\n",
    "We can now solve the surface of revolution problem. \n",
    "In this case $L(f) = x\\sqrt{1+f'}$, so $\\frac{\\partial L}{∂ f} = 0$ \n",
    "and $\\frac{\\partial L}{∂ \\dot{f}} = \\frac{xf'}{\\sqrt{1+f'}}$, so from the EL equation we get\n",
    "\n",
    "$$\n",
    "\\frac{d}{dx}\\frac{xf'}{\\sqrt{1+f'}} = 0 \\implies\\\\\n",
    "\\frac{xf'}{\\sqrt{1+f'}} = a \\implies\\\\\n",
    "f'(x) = \\frac{a}{\\sqrt{x^2-a^2}} \\implies\\\\\n",
    "f(x) = a∫ \\frac{dx}{\\sqrt{x^2-a^2}} + b \\implies\\\\\n",
    "f(x) = a \\textrm{ acosh}(x/a) + b\n",
    "$$\n",
    "\n",
    "where the constants a,b can be determined by the values of the function at the end points $x_1$ and $x_2$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb07bac",
   "metadata": {},
   "source": [
    "## Extensions\n",
    "\n",
    "**TODO** Calculus of Variations in Optimal Control\n",
    "\n",
    "**TODO** Calculus of Variations in Differential Geometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e50aea5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "computational-physics",
   "language": "python",
   "name": "computational-physics"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
