{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1324c7d",
   "metadata": {},
   "source": [
    "# Optimization and Calculus of Variations\n",
    "\n",
    "## Overview\n",
    "### 1. Optimization Primer\n",
    "### 2. The Derivative\n",
    "### 3. Calculus of Variations\n",
    "### 4. Euler Lagrange Equations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01802a20",
   "metadata": {},
   "source": [
    "## 1. Optimization Primer\n",
    "\n",
    "Optimization is the science of finding the best way to do something. Unsurprisingly, it is prevalent in many fields of engineering, science, and finance. We want to find ways to maximize profits, minimize failures, et cetera.\n",
    "The entire field of Operations Reasearch is dedicated to solving optimization problems.\n",
    "\n",
    "In it's simplest form, an optimization problem is given as a function, and we seek the point at which this function becomes a minimum or maximum.\n",
    "\n",
    "**Example**: I hold two risky securities whose risky returns have variance $\\sigma_1$ and $\\sigma_2$. The securities are correlated with covariance $c$. What proportion of each security should I hold in order to minimize the variance?\n",
    "\n",
    "**Model**: Let $w$ be the fraction that I invest in security 1. Then $(1-w)$ is the fraction of cash invested in the second security. The total variance of this portfolio is $w^2\\sigma_1^2 + (1-w)^2\\sigma_2^2+2w(1-w)c$.\n",
    "To solve the problem, I must find $w$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651f6cf9",
   "metadata": {},
   "source": [
    "### Methodology\n",
    "\n",
    "In the above example, we modeled the problem as a function of one unknown variable. We seek the the value of the variable that will result in the minimum value of a function. \n",
    "\n",
    "We should make precise what we mean by a minimum:\n",
    "\n",
    "#### Defintion: Minimum\n",
    "Let $S$ be a set and $f: S \\rightarrow \\mathbb{R}$ a function from this set to the real numbers. $f$ has a **local minimum** at $x_0 \\in S$ if there exists a neighborhood of $x_0$ such that $f(x) \\geq f(x_0) \\forall x \\in U$. \n",
    "$x_0$ is a **gobal minimum** if $f(x) \\geq f(x_0) \\forall x \\in S$.\n",
    "\n",
    "The definition for maximum is similar with the inequalities reversed. \n",
    "\n",
    "I have not yet defined what I mean by a *neighborhood*. Intuitivelly, it means a subset of $S$ consisting of points that are close to $x$. In order to determine if points are close or far, we need some measure of distance defined on the set $S$.\n",
    "Luckily, the real line, as well as any euclidean space comes equiped with a natural measure of distance.\n",
    "\n",
    "Recall that the standard norm in $\\mathbb{R}^n$ is given by $||x|| = \\sqrt{\\left(\\sum_i x_i^2\\right)}$\n",
    "and the distance between two points x and y is given by the norm of the difference \n",
    "$||x-y|| = \\sqrt{\\left(\\sum_i (x_i-y_i)^2\\right)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d57fdfe",
   "metadata": {},
   "source": [
    "The key result to solving optimization problems, is the following theorem from calculus:\n",
    "\n",
    "#### Theorem: Necessary Optimality Conditions\n",
    "Let $f: \\mathbb{R}^n \\rightarrow \\mathbb{R}$ be a continuous differentiable function.\n",
    "\n",
    "> If $f$ has a local minimum at $x_0$, then $\\nabla f(x_0) = 0$. \n",
    "\n",
    "The converse is not always true but if $f$ has a second derivative,\n",
    "a stronger condition exists to guarantee a minimum.\n",
    "\n",
    "> $f$ has a local minimum at $x_0$ if and only if, $\\nabla f(x_0) = 0$ and $\\nabla^2 f(x_0) \\geq 0$. \n",
    "\n",
    "\n",
    "Note that $\\nabla f$ stands for the vector $(\\partial{f}/\\partial{x_1}, \\ldots \\partial{f}/\\partial{x_n})$.\n",
    "In the one dimensional case this is simply the well known derivative $\\frac{df}{dx}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2cc0a74",
   "metadata": {},
   "source": [
    "The following picture illustrates a function $f: \\mathbb{R} \\rightarrow \\mathbb{R}$ and the various points where the derivative is 0.\n",
    "\n",
    "<img src='variations/Tangent_function_animation.gif'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9460df33",
   "metadata": {},
   "source": [
    "This property suggests a simple algorithm for finding the minimum: Find all points where the derivative of the function is 0. If there are multiple, compute the value of the fucntion at each one, and choose the minimum.\n",
    "\n",
    "Using this algorithm, we can now solve the variance minimization problem:\n",
    "\n",
    "$$\n",
    "f(w) = w^2\\sigma_1^2 + (1-w)^2\\sigma_2^2+2w(1-w)c\n",
    "$$\n",
    "\n",
    "Taking the derivative and doing a bit of algebra, we obtain:\n",
    "\n",
    "$$\n",
    "w_{min} = \\frac{\\sigma_2^2 - c}{\\sigma_1^2+\\sigma_2^2-2c}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd4d539",
   "metadata": {},
   "source": [
    "Even though the algorithm for finding minima by computing the points of zero gradiend only guarantees a local minimum, it works so well that is has widespread applicatins. \n",
    "A large class of Machine Learning problems are formulated as multi-variate optimization problems that are solved by computing the value of zero gradient. When it comes to Deep Learning networks, this optimization involves space of tens of thousands, even millions, of dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29bdc00d",
   "metadata": {},
   "source": [
    "## Limitations\n",
    "We have a powerful tool for calculating optimality over a large number of dimensions. Yet, there are still problems that cannot be solved with this framework as developed so far.\n",
    "\n",
    "### Example: Surface of revolution\n",
    "Consider the following problem: I draw the graph of a function $x(t): \\mathbb{R} \\rightarrow \\mathbb{R}$ between two points $t_1$ and $t_2$. Then I revolve the graph around the t-axis to create a surface.\n",
    "The area of the surface thus described has area \n",
    "$$\n",
    "A = 2\\pi\\int\\limits_{t_1}^{t_2} t\\sqrt{1+\\dot{x(t)}}dt\n",
    "$$\n",
    "\n",
    "We are interested in finding the function that results in the surface of revolution of minimal area.\n",
    "\n",
    "<img src='variations/revolution-vase.png'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51181e53",
   "metadata": {},
   "source": [
    "## 2. The Derivative\n",
    "\n",
    "A problem of the type above calls for optimizing a function of another function. Such a function is frequently called a *Functional*. We can think of a functional as a function $F: V \\rightarrow \\mathbb{R}$, where $V$ is a space of functions.\n",
    "Unlike the problems we tackled before where the domain $f: \\mathbb{R}^n \\rightarrow \\mathbb{R}$ had finite dimension, \n",
    "this new space of functions has potentially infinite dimension.\n",
    "\n",
    "Can we even take derivatives in an infinite dimensional space?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e399a7",
   "metadata": {},
   "source": [
    "The first thing to do is to look closely at the definition of the derivative and see how it can be extened.\n",
    "\n",
    "In a calculus class the derivative at point $x$ is usually defined as\n",
    "\n",
    "$$\n",
    "\\frac{df}{dx} := \\lim_{h \\rightarrow 0}\\frac{f(x+h)-f(x)}{h} \n",
    "$$\n",
    "\n",
    "Even in this simple one dimensional definition one has to be careful, because we may arrive at different result if we approach 0 from the left (h negative) or from the right (h positive).\n",
    "<img src='variations/Absolute_value.png'  width='300'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a23e31",
   "metadata": {},
   "source": [
    "So in general we need to make sure that we take the derivative in one direction, then the other and make sure that the two agree. In more dimensions, the situation becomes even trickier. Each partial derivative may exist at a given point, but the function may still fail to be differentiable at that point. \n",
    "\n",
    "There are a few ways to generalize the simple derivative from calculus to more than one dimension and even to infinite dimensional vector spaces. The \"proper\" way to define a derivative is called the *Frechet* or *Strong* derivative. \n",
    "It's definition is somewhat technical and getting into the details would bring is too far afield.\n",
    "\n",
    "Instead, I will present a generalization of the directional derivative, called the *variational derivative*, which is simpler and is what is usually used in practice.\n",
    "\n",
    "### Definition: The Variational Derivative\n",
    "Let $F: V \\rightarrow \\mathbb{R}$ be a real-valued function defined on a vector space $V$ (possibly infinite dimensional).\n",
    "The $variational derivative$ of $F$ at $x$ in the direction $h$ is given by \n",
    "\n",
    "$$\n",
    "D_hF(x) := \\lim_{\\epsilon → 0} \\frac{f(x+\\epsilon h)-f(x)}{\\epsilon}\n",
    "$$\n",
    "\n",
    "where $\\epsilon$ is a positive real number.\n",
    "\n",
    "Note that this derivative is in general dependent on the direction vector $h$. If, upon calculating the derivative,\n",
    "we find it is independent of $h$, then that's a good sign as it means that probably the derivative is well defined in every direction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1d8b12",
   "metadata": {},
   "source": [
    "## 3.  Calculus of Variations\n",
    "\n",
    "The *Calculus of Variations* is the field of computing derivatives and solving optimization problems over function spaces.\n",
    "\n",
    "Luckily, it turns out that the conditions for optimality still hold, with some care, in this case:\n",
    "\n",
    "> If $x_0$ is a local minimum of $f:V → \\mathbb{R}$, then the generilized (Frechet) derivative is 0 at $x_0$.\n",
    "\n",
    "As we mentioned above, it is sufficient to calculate the Gateaux derivative and make sure it is indepdent of the choice of \"direction\" h.\n",
    "\n",
    "### Example: Derivative of the norm.\n",
    "\n",
    "Let $V$ be a (real) Hilbert space. A Hilbert space is a vector space equiped with an inner product - and therefore a norm. Hilbert spaces of finite and infinite dimensions are omnipresent in Quantum Mechanics.\n",
    "\n",
    "Let $<v,w>$ denote the inner product of two vectors in the space, and $||v||$ the norm of the vector $v$.\n",
    "We are interested in computing the derivative of the squared norm in the direction of $h$.\n",
    "\n",
    "Applying the definition of the derivative:\n",
    "\n",
    "$$\n",
    "D_h ||x||^2 := \\lim_{\\epsilon → 0} \\frac{||x+\\epsilon h||^2-||x||^2}{\\epsilon}\n",
    "$$\n",
    "\n",
    "Now we remember that $||x||^2 = <x,x>$, so we can write\n",
    "\n",
    "$$\n",
    "||x+\\epsilon h|| = <x+\\epsilon h, x+\\epsilon h> = ||x||^2+2e<x,h>+e^2||h||^2\n",
    "$$\n",
    "\n",
    "Plugging in this in the formula for the derivative we get \n",
    "\n",
    "$$\n",
    "D_h ||x||^2 := \\lim_{\\epsilon → 0} \\frac{2e<x,h>+e^2||h||^2}{\\epsilon} =\\\\\n",
    "2<x,h>\n",
    "$$\n",
    "\n",
    "So we obtain a result consistent with finite vector spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128ccfa5",
   "metadata": {},
   "source": [
    "## 4. The Euler Lagrange Equations\n",
    "\n",
    "We are now in a position to introduce and solve the Euler Lagrange equations. Let\n",
    "\n",
    "$$\n",
    "I(x) = \\int\\limits_{t_1}^{t_2} L(x, \\dot{x})dt\n",
    "$$\n",
    "\n",
    "**Problem**: Find the function x that minimizes I.\n",
    "\n",
    "This is an optimization problem in an infinite dimensional space. \n",
    "It turns out that the situation is simular to the finite dimensional cases. \n",
    "We look for places where the derivative of I is equal to 0.\n",
    "\n",
    "As we mentioned, the Frechet derivative is hard to compute by the definition. We compute the Gateaux derivative and if independent of the direction, it is the derivative.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7edc997",
   "metadata": {},
   "source": [
    "### Preparation\n",
    "\n",
    "$x$ is a function of the variable $t$ and $\\dot{x}$ is it's derivative. $L(x, \\dot{x})$ can be viewed as a function of two variables $L(x, y)$.\n",
    "\n",
    "Recall that the Taylor expansion of any sunch function $L$ at the poing $(x+ex', y+ey')$ is given by\n",
    "\n",
    "$$\n",
    "L(x+ex', y+ey') = L(x,y) + e \\left(\\frac{\\partial L(x,y)}{\\partial x}x' + \\frac{\\partial L(x,y)}{\\partial y}y'\\right) + O(e^2)\n",
    "$$\n",
    "\n",
    "where $e$ is a small number. I can write the above as \n",
    "\n",
    "$$\n",
    "L(x+ex', y+ey') - L(x,y) =  e \\left(\\frac{\\partial L(x,y)}{\\partial x}x' + \\frac{\\partial L(x,y)}{\\partial y}y'\\right) + O(e^2)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f723ebe4",
   "metadata": {},
   "source": [
    "### Computation of the derivative\n",
    "\n",
    "Now we are going to use the definition of the variational derivative \n",
    "$\\lim_{\\epsilon → 0} \\frac{f(x+\\epsilon h)-f(x)}{\\epsilon}$.\n",
    "\n",
    "First we compute $I(x+eh)-I(x)$, where $e$ is a small number and $h$ an arbitrary function.\n",
    "It is not however completely arbitrary. \n",
    "The function h must be chose so that the value at the endpoints $t_0$ and $t_1$ remains the same. \n",
    "In other words we must have $f(x_0)+h(t_0) = f(x_0)$, from which we obtain $h(t_0) = 0$. \n",
    "The same holds for $t_1$. The idea is illustrated in the picture below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ca011b",
   "metadata": {},
   "source": [
    "<img src='variations/variations.png' width=500/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd47cef5",
   "metadata": {},
   "source": [
    "$$\n",
    "I(x+eh) - I(x) =\\\\\n",
    "\\int dt \\left(L(x+eh, \\dot{x}+e\\dot{h})-L(x,\\dot{x})\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06227229",
   "metadata": {},
   "source": [
    "Now I can use the Taylor expansion to obtain\n",
    "\n",
    "$$\n",
    "I(x+eh) - I(x) =\\\\\n",
    "e\\int dt \\left(\\frac{\\partial L}{∂ x}h + \\frac{\\partial L}{∂ \\dot{x}}\\dot{h} \\right) + O(e^2)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b426c034",
   "metadata": {},
   "source": [
    "We know that once we divide by e and take the limit $e\\rightarrow 0$, the term $O(e^2)$ is going to 0, so we are not going to worry about it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3e255b",
   "metadata": {},
   "source": [
    "We are presently concerned about the second term. We are going to use a trick from calculus\n",
    "\n",
    "$$\n",
    "\\frac{d}{dt}\\left(\\frac{\\partial L}{∂ \\dot{x}}h\\right) = \n",
    "h\\frac{d}{dt}\\left(\\frac{\\partial L}{∂ \\dot{x}}\\right) + \\dot{h}\\frac{\\partial L}{∂ \\dot{x}} \\implies\\\\\n",
    "\\dot{h}\\frac{\\partial L}{∂ \\dot{x}} = \n",
    "\\frac{d}{dt}\\left(\\frac{\\partial L}{∂ \\dot{x}}h\\right)\n",
    "- h\\frac{d}{dt}\\left(\\frac{\\partial L}{∂ \\dot{x}}\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694fb631",
   "metadata": {},
   "source": [
    "Now we note that \n",
    "\n",
    "$$\n",
    "\\int\\limits_{t_1}^{t_2} dt \\frac{d}{dt}\\left(\\frac{\\partial L}{∂ \\dot{x}}h\\right)\n",
    "= \\frac{\\partial L}{∂ \\dot{x}}h(t_2) - \\frac{\\partial L}{∂ \\dot{x}}h(t_1)\n",
    "$$\n",
    "\n",
    "But as we discussed, $h$ must live the endpoints fixed, which means $h(t_1) = h(t_2) = 0$.\n",
    "We conclude that the value of this integral is 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f6deb8",
   "metadata": {},
   "source": [
    "We are left with\n",
    "\n",
    "$$\n",
    "\\frac{I(x+eh) - I(x)}{e} = ∫ dt h \\left[\n",
    "\\frac{\\partial L}{∂ x}\n",
    "- \\frac{d}{dt}\\left(\\frac{\\partial L}{∂ \\dot{x}}\\right) \n",
    "\\right] \n",
    "+O(e^2)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4c6702",
   "metadata": {},
   "source": [
    "Taking the limit eliminates the $O(e^2)$. To find the optimum, we set the derivative to 0. \n",
    "\n",
    "$$\n",
    "∫ dt h \\left[\n",
    "\\frac{\\partial L}{∂ x}\n",
    "- \\frac{d}{dt}\\left(\\frac{\\partial L}{∂ \\dot{x}}\\right) \n",
    "\\right] = 0\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35799ddf",
   "metadata": {},
   "source": [
    "But since $h$ is a (nearly) arbitrary function, the only way for this to hold for every $h$ is if the integrant is identical to 0.\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{∂ x}\n",
    "- \\frac{d}{dt}\\left(\\frac{\\partial L}{∂ \\dot{x}}\\right)  = 0\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe2b105",
   "metadata": {},
   "source": [
    "### Application: Surface of Revolution\n",
    "\n",
    "Let's return to encounter before. We want to find a surface of revolution between two points of minimal area:\n",
    "\n",
    "$$\n",
    "A = 2\\pi\\int\\limits_{t_1}^{t_2} t\\sqrt{1+\\dot{x(t)}}dt\n",
    "$$\n",
    "\n",
    "We can now solve this problem by employing the Euler-Lagrange equations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ac38f8",
   "metadata": {},
   "source": [
    "In this case $L = t\\sqrt{1+\\dot{x(t)}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337d76d2",
   "metadata": {},
   "source": [
    "We have $\\frac{\\partial L}{∂ x}$ and\n",
    "$$\n",
    "\\frac{\\partial L}{∂ \\dot{x}} = \\frac{tx}{\\sqrt{1+\\dot{x}^2}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3cda40",
   "metadata": {},
   "source": [
    "The EL equations in this case is \n",
    "$$\n",
    "\\frac{d}{dt}\\frac{tx}{\\sqrt{1+\\dot{x}^2}} = 0 \\implies \\frac{tx}{\\sqrt{1+\\dot{x}^2}} = \\textrm{constant} \\equiv a\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5675eb",
   "metadata": {},
   "source": [
    "Rearranging we obtain\n",
    "\n",
    "$$\n",
    "\\dot{x} = \\frac{a}{\\sqrt{t^2-a^2}} \\implies\\\\\n",
    "x = \\int \\frac{adt}{\\sqrt{t^2-a^2}} \\implies\\\\\n",
    "x = a \\textrm{ acosh}(t/a) + b\n",
    "$$\n",
    "\n",
    "This type of curve is called a *catenary* and the resulting surcface of revolution a *catenoid*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95868110",
   "metadata": {},
   "source": [
    "<img src='variations/catenoid.png'/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338b7108",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "computational-physics",
   "language": "python",
   "name": "computational-physics"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
